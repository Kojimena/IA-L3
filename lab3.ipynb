{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inteligencia Artificial\n",
    "#### Laboratorio 03\n",
    "Autores:\n",
    "- Mark Albrand/ 21004\n",
    "- Jimena Hernández/ 21199\n",
    "- Melissa Pérez/ 21385"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Preguntas Teóricas\n",
    "Responda a cada de las siguientes preguntas de forma clara y lo más completamente posible.\n",
    "1. Explique la diferencia entre descenso de gradiente, descenso de gradiente por mini batches y descenso de gradiente estocástico. Asegúrese de mencionar las ventajas y desventajas de cada enfoque.\n",
    "\n",
    "    - El _descenso de gradiente_ calcula el gradiente de la función de pérdida con respecto a **todos los datos de entrenamiento**. Entre sus ventajas tiene que la convergencia es generalmente estable porque usa la información completa del conjunto de datos. Como desventaja es que puede ser computacionalmente costoso por el cálculo del gradiente en todos los datos de entrenamiento.\n",
    "\n",
    "    - El _descenso de gradiente estocástico_ necesita **un solo ejemplo de entrenamiento** aleatorio para actualizar el modelo calculando la gradiente por medio de iteraciones. Como ventaja tiene que es más eficiente en términos computacionales y es más fácil almacenarlos en memoria. Una desventaja es que pueden resultar gradientes ruidosos por su aletoriedad.\n",
    "    \n",
    "    - El _descenso de gradiente por mini batches_ **divide el conjunto de datos en pequeños lotes** para calcular el gradiente y actualizar los parámetros por cada lote. Logra un equilibrio entre la eficiencia computacional del lote de descenso de gradiente y la velocidad del descenso de gradiente estocástico, lo cual es una ventaja. Como desventaja tienq eu puede quedar atrapado en mínimos locales.\n",
    "\n",
    "    [¿Qué es el descenso de gradiente?](https://www.ibm.com/mx-es/topics/gradient-descent#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Compare y contraste técnicas de extracción de features (feature extraction) y selección de features (feature selection) en machine learning. Dé ejemplos de escenarios donde cada técnica sería más apropiada.\n",
    "\n",
    "    - _Feature extraction_ es un proceso que transforma los datos originales en un nuevo conjunto de caracterísitcas. Se enfoca en las más importantes y compactas para combinarlas, de tal forma reducir la dimensionalidad de los features. Un escenario puede ser el reconocimiento facial, porque se tienen muchas características de las cuales se reduciría la cantidad de ellas creando un nuevo conjunto para el reconomiento. \n",
    "    \n",
    "    - _Feature selection_ es un proceso de selección de subconjuntos de características del conjunto original. Básicamente elimina las catacterísticas por medio de una evaluación de importancia o redundancia de cada una. Un ejemplo, es que al tener un conjunto de datos con muchas características y donde se sospecha que algunas son irrelevantes, se puede aplicar una selección de features para tomar las relevantes.\n",
    "\n",
    "    - Comparandándolas, ambas técnicas buscan mejorar el rendimiento del modelo para reducir dimensionalidad o características irrelevantes. Su diferencia es que _feature extraction_ crea **nuevas** características a partir de las originales y _feature selection_ elige **un subconjunto** de las características originales.\n",
    "\n",
    "    [Difference Between Feature Selection and Feature Extraction](https://www.geeksforgeeks.org/difference-between-feature-selection-and-feature-extraction/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Describa la arquitectura y el funcionamiento de un perceptrón de una sola capa (un tipo de red neuronal sin backpropagation). Explique cómo aprende y la forma en la que actualiza sus parámetros.\n",
    "    - Un perceptrón de una sola capa o perceptrón simple, es un bloque de construcción básico en las redes neuronales. Consta de una capa de entrada con _n_ neuronas y otra de salida con _m_ neuronas. Aprende por medio de funciones lineales separables y de un proceso iterativo que se conforma con una capa de procesamiento, ajustes de pesos y una función de activación. Actualiza sus parámetros con un algoritmo de aprendizaje supervisado para reducir la diferencia entre la salida predicha con la real.\n",
    "\n",
    "    [El perceptrón: una red neuronal artificial para clasificar datos](https://www.economicas.uba.ar/wp-content/uploads/2016/04/Garcia-Roberto-1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Ejercicios Prácticos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1 - Gradiente Descendiente Estocástico\n",
    "Usando como polinomio: 2x^3 - 3x^2 + 5x + 3, implemente el algoritmo de gradiente descendiente estocástico, el descenso de gradiente por mini batches y el descenso de gradiente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradiente descendiente estocástico\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, -3, 5, 3]"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polinomio = [2, -3, 5, 3]  # 2x^3 - 3x^2 + 5x + 3\n",
    "polinomio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-347.0, -328.6477704295273, -310.95657421062987, -293.9140440214821, -277.5078125402582, -261.7255124451329, -246.5547764142805, -231.9832371258757, -217.99852725809262, -204.5882794891059, -191.74012649709002, -179.4417009602195, -167.68063555666862, -156.44456296461195, -145.72111586222388, -135.49792692767898, -125.76262883915166, -116.50285427481631, -107.70623591284748, -99.3604064314196, -91.4529985087071, -83.9716448228845, -76.90397805212619, -70.2376308746067, -63.96023596850044, -58.059426011981884, -52.522833683225485, -47.33809166040572, -42.49283262169702, -37.974689245273886, -33.77129420931074, -29.87028019198207, -26.2592798714623, -22.92592592592592, -19.857851033547394, -17.04268787250115, -14.468069120961687, -12.121627457103429, -9.990995559100858, -8.06380610512842, -6.327691773360579, -4.770285241971811, -3.3792191891365464, -2.1421262930292624, -1.0466392318244147, -0.08039068369647318, 0.7689866731801205, 1.513860160630898, 2.1665971004813986, 2.739564814557164, 3.2451306246837297, 3.6956618526866456, 4.103525820391447, 4.4810898496236735, 4.840721262208864, 5.194787379972564, 5.555655524740312, 5.935693018337647, 6.347267182590107, 6.802745339323238, 7.314494810362579, 7.8948829175336686, 8.556276982662041, 9.31104432757325, 10.17155227409283, 11.15016814404632, 12.259259259259263, 13.511192941557185, 14.91833651276565, 16.493057294710187, 18.24772260921634, 20.194699778109623, 22.34635612321562, 24.71505896635985, 27.313175629367862, 30.153073434065156, 33.24711970227733, 36.6076817558299, 40.24712691654841, 44.17782250625836, 48.4121358467854, 52.96243425995491, 57.84108506759252, 63.06045559152386, 68.63291315357428, 74.57082507556953, 80.88655867933493, 87.59248128669611, 94.70096021947876, 102.22436279950817, 110.17505634860998, 118.56540818860991, 127.40778564133319, 136.71455602860567, 146.49808667225255, 156.77074489409955, 167.5448980159724, 178.8329133596963, 190.6471582470971, 203.0]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "X = np.linspace(-5, 5, 100)\n",
    "y = []  # Valores de y, f(x)\n",
    "\n",
    "for x in X:\n",
    "    y.append(np.polyval(polinomio, x))\n",
    "\n",
    "#X = np.concatenate([np.ones((len(X), 1)), X], axis=1)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesos iniciales: [1.76405235 0.40015721 0.97873798 2.2408932 ]\n"
     ]
    }
   ],
   "source": [
    "w = np.random.randn(4)  # Inicialización de los pesos\n",
    "\n",
    "alpha = 0.001  # Tasa de aprendizaje\n",
    "iteraciones = 100  # Número de iteraciones\n",
    "\n",
    "print(\"Pesos iniciales:\", w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradiente_polinomio(X):\n",
    "    derivada = np.polyder(polinomio)\n",
    "    return np.polyval(derivada, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(X, y, w, alpha, iteraciones):\n",
    "    graph = np.zeros(iteraciones)\n",
    "    m = len(y)\n",
    "    for i in range(iteraciones): #iteraciones\n",
    "        randon_index = np.random.randint(0,m)\n",
    "\n",
    "        xi = X[randon_index:randon_index+1] #valor random de x\n",
    "        yi = y[randon_index:randon_index+1] #valor real del random x\n",
    "        # predicciones\n",
    "        y_pred = np.polyval(w, xi)\n",
    "        error = yi - y_pred\n",
    "\n",
    "        gradients = gradiente_polinomio(xi)\n",
    "        gradiente = np.clip(gradients, -1, 1)\n",
    "        w = w - alpha * gradiente *  error\n",
    "        return w, graph\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pasos\n",
    "1. Copiar los parámetros de la función, grados del polinomio, etc\n",
    "2. Iterar por cada epoca \n",
    "3. Elegir un elemento de X a la vez\n",
    "4. Calcular las predicciones\n",
    "5. Calcular el error\n",
    "6. Gradiante\n",
    "7. Actualizar los pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de ejecución: 0.0003409385681152344\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "start = time()\n",
    "nuevos_w, graph = stochastic_gradient_descent(X, y, w, alpha, iteraciones)\n",
    "end = time()\n",
    "print(\"Tiempo de ejecución:\", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.85280113 0.48890599 1.06748677 2.32964198]\n"
     ]
    }
   ],
   "source": [
    "print(nuevos_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.85280113 0.48890599 1.06748677 2.32964198]\n"
     ]
    }
   ],
   "source": [
    "print(nuevos_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1 - Gradiente Descendiente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradiente Descendiente \n",
    "def gradient_descent(X, y, w, alpha, iteraciones):\n",
    "    graph = np.zeros(iteraciones)\n",
    "    m = len(y)\n",
    "    for i in range(iteraciones): #iteraciones\n",
    "\n",
    "        # predicciones\n",
    "        y_pred = np.polyval(w, X[i])\n",
    "        error = y[i] - y_pred\n",
    "\n",
    "        gradients = gradiente_polinomio(X[i])\n",
    "        gradiente = np.clip(gradients, -1, 1)\n",
    "        w = w - alpha * gradiente * error\n",
    "\n",
    "    return w, graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de ejecución: 0.0032198429107666016\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "start = time()\n",
    "nuevos_w, graph = gradient_descent(X, y, w, alpha, iteraciones)\n",
    "end = time()\n",
    "print(\"Tiempo de ejecución:\", end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesos finales: [15.80180178 14.43790665 15.01648742 16.27864264]\n"
     ]
    }
   ],
   "source": [
    "print (\"Pesos finales:\", nuevos_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1 - Gradiente Descendiente mini batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def minibatch_gradient_descent(X, y, w, alpha, iteraciones, batch_size):\n",
    "    graph = np.zeros(iteraciones)\n",
    "    m = len(y)\n",
    "    n_batches = m // batch_size\n",
    "    \n",
    "    for i in range(iteraciones):\n",
    "        # Mezclar los datos para asegurar aleatoriedad en la selección de minibatches\n",
    "        indices = np.random.permutation(m)\n",
    "        X_shuffled = X[indices]\n",
    "        y_shuffled = []\n",
    "        for idx in range(len(indices)):\n",
    "            y_shuffled.append(y[idx])\n",
    "        \n",
    "        for batch in range(n_batches):\n",
    "            start_index = batch * batch_size\n",
    "            end_index = start_index + batch_size\n",
    "            X_batch = X_shuffled[start_index:end_index]\n",
    "            y_batch = y_shuffled[start_index:end_index]\n",
    "            \n",
    "            # Calcular predicciones para el minibatch\n",
    "            y_pred = np.polyval(w, X_batch)\n",
    "            \n",
    "            # Calcular el error para el minibatch\n",
    "            error = y_batch - y_pred\n",
    "            \n",
    "            # Calcular el gradiente para el minibatch\n",
    "            gradients = np.mean([gradiente_polinomio(x) for x in X_batch], axis=0)\n",
    "            gradiente = np.clip(gradients, -1, 1)\n",
    "            \n",
    "            # Actualizar los pesos\n",
    "            w = w - alpha * gradiente * np.mean(error)\n",
    "            \n",
    "            # Opcional: guardar o calcular métricas de interés en 'graph' para su posterior análisis\n",
    "            \n",
    "    return w, graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo de ejecución: 0.044519901275634766\n",
      "[14.1944435  13.08083533 12.72596428 13.03203524]\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "start = time()\n",
    "nuevos_w, graph = minibatch_gradient_descent(X, y, w, alpha, iteraciones, 55)\n",
    "end = time()\n",
    "\n",
    "print(\"Tiempo de ejecución:\", end - start)\n",
    "\n",
    "print(nuevos_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tiempos de ejecución\n",
    "\n",
    "- Gradiente Descendiente Estocástico : 0.00021\n",
    "- Gradiente Descendiente : 0.0028\n",
    "- Gradiente Descendiente Minibatches: 0.0445\n",
    "\n",
    "El método más rápido fue Gradiente Descendiente Estocástico con 0.00021 esto debido a que solo toma un valor al azar y no el dataset completo, ya que esto es mas costoso al igual que la realización de los minibatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2 - Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Método de filtrado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gameId</th>\n",
       "      <th>blueWins</th>\n",
       "      <th>blueWardsPlaced</th>\n",
       "      <th>blueWardsDestroyed</th>\n",
       "      <th>blueFirstBlood</th>\n",
       "      <th>blueKills</th>\n",
       "      <th>blueDeaths</th>\n",
       "      <th>blueAssists</th>\n",
       "      <th>blueEliteMonsters</th>\n",
       "      <th>blueDragons</th>\n",
       "      <th>...</th>\n",
       "      <th>redTowersDestroyed</th>\n",
       "      <th>redTotalGold</th>\n",
       "      <th>redAvgLevel</th>\n",
       "      <th>redTotalExperience</th>\n",
       "      <th>redTotalMinionsKilled</th>\n",
       "      <th>redTotalJungleMinionsKilled</th>\n",
       "      <th>redGoldDiff</th>\n",
       "      <th>redExperienceDiff</th>\n",
       "      <th>redCSPerMin</th>\n",
       "      <th>redGoldPerMin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4519157822</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>16567</td>\n",
       "      <td>6.8</td>\n",
       "      <td>17047</td>\n",
       "      <td>197</td>\n",
       "      <td>55</td>\n",
       "      <td>-643</td>\n",
       "      <td>8</td>\n",
       "      <td>19.7</td>\n",
       "      <td>1656.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4523371949</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>17620</td>\n",
       "      <td>6.8</td>\n",
       "      <td>17438</td>\n",
       "      <td>240</td>\n",
       "      <td>52</td>\n",
       "      <td>2908</td>\n",
       "      <td>1173</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1762.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4521474530</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>17285</td>\n",
       "      <td>6.8</td>\n",
       "      <td>17254</td>\n",
       "      <td>203</td>\n",
       "      <td>28</td>\n",
       "      <td>1172</td>\n",
       "      <td>1033</td>\n",
       "      <td>20.3</td>\n",
       "      <td>1728.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4524384067</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>16478</td>\n",
       "      <td>7.0</td>\n",
       "      <td>17961</td>\n",
       "      <td>235</td>\n",
       "      <td>47</td>\n",
       "      <td>1321</td>\n",
       "      <td>7</td>\n",
       "      <td>23.5</td>\n",
       "      <td>1647.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4436033771</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>17404</td>\n",
       "      <td>7.0</td>\n",
       "      <td>18313</td>\n",
       "      <td>225</td>\n",
       "      <td>67</td>\n",
       "      <td>1004</td>\n",
       "      <td>-230</td>\n",
       "      <td>22.5</td>\n",
       "      <td>1740.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9874</th>\n",
       "      <td>4527873286</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>15246</td>\n",
       "      <td>6.8</td>\n",
       "      <td>16498</td>\n",
       "      <td>229</td>\n",
       "      <td>34</td>\n",
       "      <td>-2519</td>\n",
       "      <td>-2469</td>\n",
       "      <td>22.9</td>\n",
       "      <td>1524.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9875</th>\n",
       "      <td>4527797466</td>\n",
       "      <td>1</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>15456</td>\n",
       "      <td>7.0</td>\n",
       "      <td>18367</td>\n",
       "      <td>206</td>\n",
       "      <td>56</td>\n",
       "      <td>-782</td>\n",
       "      <td>-888</td>\n",
       "      <td>20.6</td>\n",
       "      <td>1545.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9876</th>\n",
       "      <td>4527713716</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>18319</td>\n",
       "      <td>7.4</td>\n",
       "      <td>19909</td>\n",
       "      <td>261</td>\n",
       "      <td>60</td>\n",
       "      <td>2416</td>\n",
       "      <td>1877</td>\n",
       "      <td>26.1</td>\n",
       "      <td>1831.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9877</th>\n",
       "      <td>4527628313</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>15298</td>\n",
       "      <td>7.2</td>\n",
       "      <td>18314</td>\n",
       "      <td>247</td>\n",
       "      <td>40</td>\n",
       "      <td>839</td>\n",
       "      <td>1085</td>\n",
       "      <td>24.7</td>\n",
       "      <td>1529.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9878</th>\n",
       "      <td>4523772935</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>15339</td>\n",
       "      <td>6.8</td>\n",
       "      <td>17379</td>\n",
       "      <td>201</td>\n",
       "      <td>46</td>\n",
       "      <td>-927</td>\n",
       "      <td>58</td>\n",
       "      <td>20.1</td>\n",
       "      <td>1533.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9879 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          gameId  blueWins  blueWardsPlaced  blueWardsDestroyed  \\\n",
       "0     4519157822         0               28                   2   \n",
       "1     4523371949         0               12                   1   \n",
       "2     4521474530         0               15                   0   \n",
       "3     4524384067         0               43                   1   \n",
       "4     4436033771         0               75                   4   \n",
       "...          ...       ...              ...                 ...   \n",
       "9874  4527873286         1               17                   2   \n",
       "9875  4527797466         1               54                   0   \n",
       "9876  4527713716         0               23                   1   \n",
       "9877  4527628313         0               14                   4   \n",
       "9878  4523772935         1               18                   0   \n",
       "\n",
       "      blueFirstBlood  blueKills  blueDeaths  blueAssists  blueEliteMonsters  \\\n",
       "0                  1          9           6           11                  0   \n",
       "1                  0          5           5            5                  0   \n",
       "2                  0          7          11            4                  1   \n",
       "3                  0          4           5            5                  1   \n",
       "4                  0          6           6            6                  0   \n",
       "...              ...        ...         ...          ...                ...   \n",
       "9874               1          7           4            5                  1   \n",
       "9875               0          6           4            8                  1   \n",
       "9876               0          6           7            5                  0   \n",
       "9877               1          2           3            3                  1   \n",
       "9878               1          6           6            5                  0   \n",
       "\n",
       "      blueDragons  ...  redTowersDestroyed  redTotalGold  redAvgLevel  \\\n",
       "0               0  ...                   0         16567          6.8   \n",
       "1               0  ...                   1         17620          6.8   \n",
       "2               1  ...                   0         17285          6.8   \n",
       "3               0  ...                   0         16478          7.0   \n",
       "4               0  ...                   0         17404          7.0   \n",
       "...           ...  ...                 ...           ...          ...   \n",
       "9874            1  ...                   0         15246          6.8   \n",
       "9875            1  ...                   0         15456          7.0   \n",
       "9876            0  ...                   0         18319          7.4   \n",
       "9877            1  ...                   0         15298          7.2   \n",
       "9878            0  ...                   0         15339          6.8   \n",
       "\n",
       "      redTotalExperience  redTotalMinionsKilled  redTotalJungleMinionsKilled  \\\n",
       "0                  17047                    197                           55   \n",
       "1                  17438                    240                           52   \n",
       "2                  17254                    203                           28   \n",
       "3                  17961                    235                           47   \n",
       "4                  18313                    225                           67   \n",
       "...                  ...                    ...                          ...   \n",
       "9874               16498                    229                           34   \n",
       "9875               18367                    206                           56   \n",
       "9876               19909                    261                           60   \n",
       "9877               18314                    247                           40   \n",
       "9878               17379                    201                           46   \n",
       "\n",
       "      redGoldDiff  redExperienceDiff  redCSPerMin  redGoldPerMin  \n",
       "0            -643                  8         19.7         1656.7  \n",
       "1            2908               1173         24.0         1762.0  \n",
       "2            1172               1033         20.3         1728.5  \n",
       "3            1321                  7         23.5         1647.8  \n",
       "4            1004               -230         22.5         1740.4  \n",
       "...           ...                ...          ...            ...  \n",
       "9874        -2519              -2469         22.9         1524.6  \n",
       "9875         -782               -888         20.6         1545.6  \n",
       "9876         2416               1877         26.1         1831.9  \n",
       "9877          839               1085         24.7         1529.8  \n",
       "9878         -927                 58         20.1         1533.9  \n",
       "\n",
       "[9879 rows x 40 columns]"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('lol.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gameId                          0\n",
       "blueWins                        0\n",
       "blueWardsPlaced                 0\n",
       "blueWardsDestroyed              0\n",
       "blueFirstBlood                  0\n",
       "blueKills                       0\n",
       "blueDeaths                      0\n",
       "blueAssists                     0\n",
       "blueEliteMonsters               0\n",
       "blueDragons                     0\n",
       "blueHeralds                     0\n",
       "blueTowersDestroyed             0\n",
       "blueTotalGold                   0\n",
       "blueAvgLevel                    0\n",
       "blueTotalExperience             0\n",
       "blueTotalMinionsKilled          0\n",
       "blueTotalJungleMinionsKilled    0\n",
       "blueGoldDiff                    0\n",
       "blueExperienceDiff              0\n",
       "blueCSPerMin                    0\n",
       "blueGoldPerMin                  0\n",
       "redWardsPlaced                  0\n",
       "redWardsDestroyed               0\n",
       "redFirstBlood                   0\n",
       "redKills                        0\n",
       "redDeaths                       0\n",
       "redAssists                      0\n",
       "redEliteMonsters                0\n",
       "redDragons                      0\n",
       "redHeralds                      0\n",
       "redTowersDestroyed              0\n",
       "redTotalGold                    0\n",
       "redAvgLevel                     0\n",
       "redTotalExperience              0\n",
       "redTotalMinionsKilled           0\n",
       "redTotalJungleMinionsKilled     0\n",
       "redGoldDiff                     0\n",
       "redExperienceDiff               0\n",
       "redCSPerMin                     0\n",
       "redGoldPerMin                   0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ver si los datos tienen valores nulos\n",
    "data.isnull().sum() #esta balanceado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['blueWins']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gameId</th>\n",
       "      <th>blueWardsPlaced</th>\n",
       "      <th>blueWardsDestroyed</th>\n",
       "      <th>blueFirstBlood</th>\n",
       "      <th>blueKills</th>\n",
       "      <th>blueDeaths</th>\n",
       "      <th>blueAssists</th>\n",
       "      <th>blueEliteMonsters</th>\n",
       "      <th>blueDragons</th>\n",
       "      <th>blueHeralds</th>\n",
       "      <th>...</th>\n",
       "      <th>redTowersDestroyed</th>\n",
       "      <th>redTotalGold</th>\n",
       "      <th>redAvgLevel</th>\n",
       "      <th>redTotalExperience</th>\n",
       "      <th>redTotalMinionsKilled</th>\n",
       "      <th>redTotalJungleMinionsKilled</th>\n",
       "      <th>redGoldDiff</th>\n",
       "      <th>redExperienceDiff</th>\n",
       "      <th>redCSPerMin</th>\n",
       "      <th>redGoldPerMin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4519157822</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>16567</td>\n",
       "      <td>6.8</td>\n",
       "      <td>17047</td>\n",
       "      <td>197</td>\n",
       "      <td>55</td>\n",
       "      <td>-643</td>\n",
       "      <td>8</td>\n",
       "      <td>19.7</td>\n",
       "      <td>1656.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4523371949</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>17620</td>\n",
       "      <td>6.8</td>\n",
       "      <td>17438</td>\n",
       "      <td>240</td>\n",
       "      <td>52</td>\n",
       "      <td>2908</td>\n",
       "      <td>1173</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1762.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4521474530</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>17285</td>\n",
       "      <td>6.8</td>\n",
       "      <td>17254</td>\n",
       "      <td>203</td>\n",
       "      <td>28</td>\n",
       "      <td>1172</td>\n",
       "      <td>1033</td>\n",
       "      <td>20.3</td>\n",
       "      <td>1728.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4524384067</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>16478</td>\n",
       "      <td>7.0</td>\n",
       "      <td>17961</td>\n",
       "      <td>235</td>\n",
       "      <td>47</td>\n",
       "      <td>1321</td>\n",
       "      <td>7</td>\n",
       "      <td>23.5</td>\n",
       "      <td>1647.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4436033771</td>\n",
       "      <td>75</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>17404</td>\n",
       "      <td>7.0</td>\n",
       "      <td>18313</td>\n",
       "      <td>225</td>\n",
       "      <td>67</td>\n",
       "      <td>1004</td>\n",
       "      <td>-230</td>\n",
       "      <td>22.5</td>\n",
       "      <td>1740.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       gameId  blueWardsPlaced  blueWardsDestroyed  blueFirstBlood  blueKills  \\\n",
       "0  4519157822               28                   2               1          9   \n",
       "1  4523371949               12                   1               0          5   \n",
       "2  4521474530               15                   0               0          7   \n",
       "3  4524384067               43                   1               0          4   \n",
       "4  4436033771               75                   4               0          6   \n",
       "\n",
       "   blueDeaths  blueAssists  blueEliteMonsters  blueDragons  blueHeralds  ...  \\\n",
       "0           6           11                  0            0            0  ...   \n",
       "1           5            5                  0            0            0  ...   \n",
       "2          11            4                  1            1            0  ...   \n",
       "3           5            5                  1            0            1  ...   \n",
       "4           6            6                  0            0            0  ...   \n",
       "\n",
       "   redTowersDestroyed  redTotalGold  redAvgLevel  redTotalExperience  \\\n",
       "0                   0         16567          6.8               17047   \n",
       "1                   1         17620          6.8               17438   \n",
       "2                   0         17285          6.8               17254   \n",
       "3                   0         16478          7.0               17961   \n",
       "4                   0         17404          7.0               18313   \n",
       "\n",
       "   redTotalMinionsKilled  redTotalJungleMinionsKilled  redGoldDiff  \\\n",
       "0                    197                           55         -643   \n",
       "1                    240                           52         2908   \n",
       "2                    203                           28         1172   \n",
       "3                    235                           47         1321   \n",
       "4                    225                           67         1004   \n",
       "\n",
       "   redExperienceDiff  redCSPerMin  redGoldPerMin  \n",
       "0                  8         19.7         1656.7  \n",
       "1               1173         24.0         1762.0  \n",
       "2               1033         20.3         1728.5  \n",
       "3                  7         23.5         1647.8  \n",
       "4               -230         22.5         1740.4  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data.drop('blueWins', axis=1)\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlaciones = {}\n",
    "for col in X.columns:\n",
    "    corr_pearson = np.corrcoef(X[col], y)[0,1]\n",
    "    correlaciones[col] = corr_pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('blueGoldDiff', 0.51111905517625),\n",
       " ('blueExperienceDiff', 0.48955791655969777),\n",
       " ('blueTotalGold', 0.41721255829217974),\n",
       " ('blueGoldPerMin', 0.41721255829217946),\n",
       " ('blueTotalExperience', 0.3961407056514389),\n",
       " ('blueAvgLevel', 0.3578198468989065),\n",
       " ('blueKills', 0.3373576105342435),\n",
       " ('redDeaths', 0.3373576105342435),\n",
       " ('blueAssists', 0.2766849645392132),\n",
       " ('blueCSPerMin', 0.22490947260651514),\n",
       " ('blueTotalMinionsKilled', 0.2249094726065151),\n",
       " ('blueEliteMonsters', 0.2219441945274376),\n",
       " ('blueDragons', 0.21376769276290752),\n",
       " ('blueFirstBlood', 0.2017692643857104),\n",
       " ('blueTotalJungleMinionsKilled', 0.13144491466164793),\n",
       " ('blueTowersDestroyed', 0.11556646320823924),\n",
       " ('blueHeralds', 0.0923847247473175),\n",
       " ('blueWardsDestroyed', 0.044246803571579015),\n",
       " ('gameId', 0.0009851278627279335),\n",
       " ('blueWardsPlaced', 8.695109201392437e-05),\n",
       " ('redWardsPlaced', -0.023671238287740017),\n",
       " ('redWardsDestroyed', -0.05540030842294096),\n",
       " ('redHeralds', -0.09717188056275644),\n",
       " ('redTowersDestroyed', -0.10369562570246864),\n",
       " ('redTotalJungleMinionsKilled', -0.1109935445795713),\n",
       " ('redFirstBlood', -0.2017692643857104),\n",
       " ('redDragons', -0.2095159015790154),\n",
       " ('redCSPerMin', -0.21217146445217194),\n",
       " ('redTotalMinionsKilled', -0.212171464452172),\n",
       " ('redEliteMonsters', -0.22155108781857924),\n",
       " ('redAssists', -0.2710469185309071),\n",
       " ('blueDeaths', -0.3392967410875846),\n",
       " ('redKills', -0.3392967410875846),\n",
       " ('redAvgLevel', -0.3521267631821689),\n",
       " ('redTotalExperience', -0.3875875647261892),\n",
       " ('redTotalGold', -0.4113962473532144),\n",
       " ('redGoldPerMin', -0.4113962473532144),\n",
       " ('redExperienceDiff', -0.48955791655969777),\n",
       " ('redGoldDiff', -0.51111905517625)]"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ordenar correlaciones de mayor a menor\n",
    "correlaciones_ordenadas = sorted(correlaciones.items(), key=lambda x: x[1], reverse=True)\n",
    "correlaciones_ordenadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Top 5 features con método de filtrado:\n",
    "- BlueGoldDiff\n",
    "- BlueExperienceDiff\n",
    "- BlueTotalGold\n",
    "- BlueGoldperMin\n",
    "- BlueTotalExperience\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.72165991902834"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM con los features elegidos\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_svm = X[['blueGoldDiff', 'blueExperienceDiff', 'blueTotalGold', 'blueGoldPerMin', 'blueTotalExperience']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_svm, y, test_size=0.2, random_state=42)\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "y_pred = svm.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Método Wrapper\n",
    "Usando backwar eliminationds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gameId</th>\n",
       "      <th>blueWardsPlaced</th>\n",
       "      <th>blueWardsDestroyed</th>\n",
       "      <th>blueFirstBlood</th>\n",
       "      <th>blueKills</th>\n",
       "      <th>blueDeaths</th>\n",
       "      <th>blueAssists</th>\n",
       "      <th>blueEliteMonsters</th>\n",
       "      <th>blueDragons</th>\n",
       "      <th>blueHeralds</th>\n",
       "      <th>...</th>\n",
       "      <th>redTowersDestroyed</th>\n",
       "      <th>redTotalGold</th>\n",
       "      <th>redAvgLevel</th>\n",
       "      <th>redTotalExperience</th>\n",
       "      <th>redTotalMinionsKilled</th>\n",
       "      <th>redTotalJungleMinionsKilled</th>\n",
       "      <th>redGoldDiff</th>\n",
       "      <th>redExperienceDiff</th>\n",
       "      <th>redCSPerMin</th>\n",
       "      <th>redGoldPerMin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.962031</td>\n",
       "      <td>0.093878</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.379310</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.464844</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.557608</td>\n",
       "      <td>0.494505</td>\n",
       "      <td>0.579545</td>\n",
       "      <td>0.485446</td>\n",
       "      <td>0.472598</td>\n",
       "      <td>0.494505</td>\n",
       "      <td>0.464844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.980146</td>\n",
       "      <td>0.028571</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.556250</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.590732</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.644706</td>\n",
       "      <td>0.538488</td>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.556250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.971990</td>\n",
       "      <td>0.040816</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.527170</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.575144</td>\n",
       "      <td>0.527473</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.566848</td>\n",
       "      <td>0.530570</td>\n",
       "      <td>0.527473</td>\n",
       "      <td>0.527170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.984497</td>\n",
       "      <td>0.155102</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.457118</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.635039</td>\n",
       "      <td>0.703297</td>\n",
       "      <td>0.488636</td>\n",
       "      <td>0.573530</td>\n",
       "      <td>0.472541</td>\n",
       "      <td>0.703297</td>\n",
       "      <td>0.457118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.604712</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.537500</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.664859</td>\n",
       "      <td>0.648352</td>\n",
       "      <td>0.715909</td>\n",
       "      <td>0.559313</td>\n",
       "      <td>0.459137</td>\n",
       "      <td>0.648352</td>\n",
       "      <td>0.537500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     gameId  blueWardsPlaced  blueWardsDestroyed  blueFirstBlood  blueKills  \\\n",
       "0  0.962031         0.093878            0.074074             1.0   0.409091   \n",
       "1  0.980146         0.028571            0.037037             0.0   0.227273   \n",
       "2  0.971990         0.040816            0.000000             0.0   0.318182   \n",
       "3  0.984497         0.155102            0.037037             0.0   0.181818   \n",
       "4  0.604712         0.285714            0.148148             0.0   0.272727   \n",
       "\n",
       "   blueDeaths  blueAssists  blueEliteMonsters  blueDragons  blueHeralds  ...  \\\n",
       "0    0.272727     0.379310                0.0          0.0          0.0  ...   \n",
       "1    0.227273     0.172414                0.0          0.0          0.0  ...   \n",
       "2    0.500000     0.137931                0.5          1.0          0.0  ...   \n",
       "3    0.227273     0.172414                0.5          0.0          1.0  ...   \n",
       "4    0.272727     0.206897                0.0          0.0          0.0  ...   \n",
       "\n",
       "   redTowersDestroyed  redTotalGold  redAvgLevel  redTotalExperience  \\\n",
       "0                 0.0      0.464844     0.588235            0.557608   \n",
       "1                 0.5      0.556250     0.588235            0.590732   \n",
       "2                 0.0      0.527170     0.588235            0.575144   \n",
       "3                 0.0      0.457118     0.647059            0.635039   \n",
       "4                 0.0      0.537500     0.647059            0.664859   \n",
       "\n",
       "   redTotalMinionsKilled  redTotalJungleMinionsKilled  redGoldDiff  \\\n",
       "0               0.494505                     0.579545     0.485446   \n",
       "1               0.730769                     0.545455     0.644706   \n",
       "2               0.527473                     0.272727     0.566848   \n",
       "3               0.703297                     0.488636     0.573530   \n",
       "4               0.648352                     0.715909     0.559313   \n",
       "\n",
       "   redExperienceDiff  redCSPerMin  redGoldPerMin  \n",
       "0           0.472598     0.494505       0.464844  \n",
       "1           0.538488     0.730769       0.556250  \n",
       "2           0.530570     0.527473       0.527170  \n",
       "3           0.472541     0.703297       0.457118  \n",
       "4           0.459137     0.648352       0.537500  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "df_escalated  = data.drop('blueWins', axis=1)\n",
    "\n",
    "for col in df_escalated.columns:\n",
    "    df_escalated[col] = MinMaxScaler().fit_transform(df_escalated[col].values.reshape(-1,1))\n",
    "\n",
    "df_escalated.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[375], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m all_features:\n\u001b[1;32m      9\u001b[0m     candidate_features \u001b[39m=\u001b[39m [f \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m all_features \u001b[39mif\u001b[39;00m f \u001b[39m!=\u001b[39m feature]\n\u001b[0;32m---> 11\u001b[0m     scores \u001b[39m=\u001b[39m cross_val_score(model, df_escalated[candidate_features], y, cv\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n\u001b[1;32m     12\u001b[0m     mean_score \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(scores)\n\u001b[1;32m     14\u001b[0m     \u001b[39mif\u001b[39;00m mean_score \u001b[39m<\u001b[39m worst_score:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:562\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[39m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m    560\u001b[0m scorer \u001b[39m=\u001b[39m check_scoring(estimator, scoring\u001b[39m=\u001b[39mscoring)\n\u001b[0;32m--> 562\u001b[0m cv_results \u001b[39m=\u001b[39m cross_validate(\n\u001b[1;32m    563\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m    564\u001b[0m     X\u001b[39m=\u001b[39;49mX,\n\u001b[1;32m    565\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    566\u001b[0m     groups\u001b[39m=\u001b[39;49mgroups,\n\u001b[1;32m    567\u001b[0m     scoring\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mscore\u001b[39;49m\u001b[39m\"\u001b[39;49m: scorer},\n\u001b[1;32m    568\u001b[0m     cv\u001b[39m=\u001b[39;49mcv,\n\u001b[1;32m    569\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    570\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    571\u001b[0m     fit_params\u001b[39m=\u001b[39;49mfit_params,\n\u001b[1;32m    572\u001b[0m     pre_dispatch\u001b[39m=\u001b[39;49mpre_dispatch,\n\u001b[1;32m    573\u001b[0m     error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[1;32m    574\u001b[0m )\n\u001b[1;32m    575\u001b[0m \u001b[39mreturn\u001b[39;00m cv_results[\u001b[39m\"\u001b[39m\u001b[39mtest_score\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m    210\u001b[0m         skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    212\u001b[0m         )\n\u001b[1;32m    213\u001b[0m     ):\n\u001b[0;32m--> 214\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    215\u001b[0m \u001b[39mexcept\u001b[39;00m InvalidParameterError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    216\u001b[0m     \u001b[39m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[39m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[39m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[39m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     msg \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[1;32m    221\u001b[0m         \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+ must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparameter of \u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    223\u001b[0m         \u001b[39mstr\u001b[39m(e),\n\u001b[1;32m    224\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:309\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[39m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[39m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    308\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39mn_jobs, verbose\u001b[39m=\u001b[39mverbose, pre_dispatch\u001b[39m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 309\u001b[0m results \u001b[39m=\u001b[39m parallel(\n\u001b[1;32m    310\u001b[0m     delayed(_fit_and_score)(\n\u001b[1;32m    311\u001b[0m         clone(estimator),\n\u001b[1;32m    312\u001b[0m         X,\n\u001b[1;32m    313\u001b[0m         y,\n\u001b[1;32m    314\u001b[0m         scorers,\n\u001b[1;32m    315\u001b[0m         train,\n\u001b[1;32m    316\u001b[0m         test,\n\u001b[1;32m    317\u001b[0m         verbose,\n\u001b[1;32m    318\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    319\u001b[0m         fit_params,\n\u001b[1;32m    320\u001b[0m         return_train_score\u001b[39m=\u001b[39;49mreturn_train_score,\n\u001b[1;32m    321\u001b[0m         return_times\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    322\u001b[0m         return_estimator\u001b[39m=\u001b[39;49mreturn_estimator,\n\u001b[1;32m    323\u001b[0m         error_score\u001b[39m=\u001b[39;49merror_score,\n\u001b[1;32m    324\u001b[0m     )\n\u001b[1;32m    325\u001b[0m     \u001b[39mfor\u001b[39;49;00m train, test \u001b[39min\u001b[39;49;00m indices\n\u001b[1;32m    326\u001b[0m )\n\u001b[1;32m    328\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    330\u001b[0m \u001b[39m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[39m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[39m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39m(output)\n\u001b[1;32m   1865\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1793\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[1;32m    126\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[0;32m--> 127\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:729\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    727\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m    728\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 729\u001b[0m         estimator\u001b[39m.\u001b[39;49mfit(X_train, y_train, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\n\u001b[1;32m    731\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    732\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    733\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:456\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    445\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[1;32m    446\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[1;32m    447\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    448\u001b[0m ]\n\u001b[1;32m    450\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 456\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[1;32m    457\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[1;32m    458\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m    459\u001b[0m     prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    460\u001b[0m )(\n\u001b[1;32m    461\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[1;32m    462\u001b[0m         t,\n\u001b[1;32m    463\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbootstrap,\n\u001b[1;32m    464\u001b[0m         X,\n\u001b[1;32m    465\u001b[0m         y,\n\u001b[1;32m    466\u001b[0m         sample_weight,\n\u001b[1;32m    467\u001b[0m         i,\n\u001b[1;32m    468\u001b[0m         \u001b[39mlen\u001b[39;49m(trees),\n\u001b[1;32m    469\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m    470\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[1;32m    471\u001b[0m         n_samples_bootstrap\u001b[39m=\u001b[39;49mn_samples_bootstrap,\n\u001b[1;32m    472\u001b[0m     )\n\u001b[1;32m    473\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, t \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(trees)\n\u001b[1;32m    474\u001b[0m )\n\u001b[1;32m    476\u001b[0m \u001b[39m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    477\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[39mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39m(output)\n\u001b[1;32m   1865\u001b[0m \u001b[39m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[39m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[39m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[39m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[39m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_dispatched_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1793\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_completed_tasks \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[1;32m    126\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[0;32m--> 127\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:188\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[39melif\u001b[39;00m class_weight \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbalanced_subsample\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    186\u001b[0m         curr_sample_weight \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m compute_sample_weight(\u001b[39m\"\u001b[39m\u001b[39mbalanced\u001b[39m\u001b[39m\"\u001b[39m, y, indices\u001b[39m=\u001b[39mindices)\n\u001b[0;32m--> 188\u001b[0m     tree\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49mcurr_sample_weight, check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    189\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     tree\u001b[39m.\u001b[39mfit(X, y, sample_weight\u001b[39m=\u001b[39msample_weight, check_input\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/tree/_classes.py:959\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[39m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    929\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    930\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[1;32m    931\u001b[0m \n\u001b[1;32m    932\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    956\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m    957\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 959\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_fit(\n\u001b[1;32m    960\u001b[0m         X,\n\u001b[1;32m    961\u001b[0m         y,\n\u001b[1;32m    962\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m    963\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[1;32m    964\u001b[0m     )\n\u001b[1;32m    965\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/tree/_classes.py:443\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    433\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    434\u001b[0m         splitter,\n\u001b[1;32m    435\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    441\u001b[0m     )\n\u001b[0;32m--> 443\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight, missing_values_in_feature_mask)\n\u001b[1;32m    445\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[1;32m    446\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_features = df_escalated.columns\n",
    "min_features = 5\n",
    "\n",
    "while len(all_features) >= min_features:\n",
    "    worst_score = 1\n",
    "    worst_feature = None\n",
    "\n",
    "    for feature in all_features:\n",
    "        candidate_features = [f for f in all_features if f != feature]\n",
    "\n",
    "        scores = cross_val_score(model, df_escalated[candidate_features], y, cv=5)\n",
    "        mean_score = np.mean(scores)\n",
    "\n",
    "        if mean_score < worst_score:\n",
    "            worst_score = mean_score\n",
    "            worst_feature = feature\n",
    "\n",
    "    print(f'Removing {worst_feature} with score {worst_score}')\n",
    "    all_features = [f for f in all_features if f != worst_feature]\n",
    "\n",
    "print(all_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Método Lasso (Embebidos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7903, 39) (1976, 39)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['blueKills', 'blueDeaths', 'blueAssists', 'blueHeralds', 'blueAvgLevel',\n",
       "       'blueTotalExperience', 'redWardsDestroyed', 'redKills', 'redDeaths',\n",
       "       'redHeralds', 'redTotalGold', 'redGoldPerMin'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 379,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = pd.read_csv('lol.csv')\n",
    "\n",
    "y = data['blueWins']\n",
    "X = data.drop('blueWins', axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print (X_train.shape, X_test.shape)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "sel_ = SelectFromModel(Lasso(alpha=0.001, random_state=10))\n",
    "sel_.fit(scaler.transform(X_train), y_train)\n",
    "\n",
    "sel_.get_support()\n",
    "\n",
    "sel_.get_feature_names_out()\n",
    "\n",
    "removed_feats = X_train.columns[(sel_.estimator_.coef_ == 0).ravel().tolist()]\n",
    "removed_feats\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7903, 27), (1976, 27))"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reducir el dataset a las features seleccionadas\n",
    "X_train_selected = sel_.transform(scaler.transform(X_train))\n",
    "X_test_selected = sel_.transform(scaler.transform(X_test))\n",
    "\n",
    "X_train_selected.shape, X_test_selected.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [7903, 80]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[383], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m accuracy_score\n\u001b[1;32m      7\u001b[0m svm \u001b[39m=\u001b[39m SVC()\n\u001b[0;32m----> 8\u001b[0m svm\u001b[39m.\u001b[39;49mfit(X_train_selected, y_train)\n\u001b[1;32m      9\u001b[0m y_pred \u001b[39m=\u001b[39m svm\u001b[39m.\u001b[39mpredict(X_test_selected)\n\u001b[1;32m     10\u001b[0m accuracy_score(y_test, y_pred)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/svm/_base.py:190\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    188\u001b[0m     check_consistent_length(X, y)\n\u001b[1;32m    189\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m     X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m    191\u001b[0m         X,\n\u001b[1;32m    192\u001b[0m         y,\n\u001b[1;32m    193\u001b[0m         dtype\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mfloat64,\n\u001b[1;32m    194\u001b[0m         order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    195\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    196\u001b[0m         accept_large_sparse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    197\u001b[0m     )\n\u001b[1;32m    199\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_targets(y)\n\u001b[1;32m    201\u001b[0m sample_weight \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(\n\u001b[1;32m    202\u001b[0m     [] \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m sample_weight, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat64\n\u001b[1;32m    203\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:622\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    620\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[1;32m    621\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 622\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    623\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/validation.py:1164\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1146\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m   1147\u001b[0m     X,\n\u001b[1;32m   1148\u001b[0m     accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1159\u001b[0m     input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1160\u001b[0m )\n\u001b[1;32m   1162\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[0;32m-> 1164\u001b[0m check_consistent_length(X, y)\n\u001b[1;32m   1166\u001b[0m \u001b[39mreturn\u001b[39;00m X, y\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/utils/validation.py:407\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    405\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[1;32m    406\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 407\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    408\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    409\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[1;32m    410\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [7903, 80]"
     ]
    }
   ],
   "source": [
    "# SVM con los features elegidos\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train_selected, y_train)\n",
    "y_pred = svm.predict(X_test_selected)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score:  0.6771255060728745\n"
     ]
    }
   ],
   "source": [
    "# insert on a dataframe called pd the following columns: blueKills', 'blueAvgLevel', 'blueWins'\n",
    "pd = data[['blueKills', 'blueAvgLevel', 'blueWins']]\n",
    "\n",
    "X = pd.iloc[:, :-1].values\n",
    "y = pd.iloc[:, -1].values\n",
    "\n",
    "#Dividir el conjunto de datos en entrenamiento y prueba\n",
    "X_entreno, X_prueba, y_entreno, y_prueba = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "\n",
    "# Entrene el modelo de SVM con el conjunto de entrenamiento\n",
    "model = SVC(kernel='linear')\n",
    "model.fit(X_entreno, y_entreno)\n",
    "\n",
    "# Predicción y métricas de evaluación\n",
    "y_pred = model.predict(X_prueba)\n",
    "print(\"accuracy_score: \", accuracy_score(y_prueba, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusión\n",
    "\n",
    "Debido a que los datos estan balanceados, utilizamos accuracy como metrica de desempeño y obtuvimos que el mejor método fue "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3 - Perceptrón"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X_filtered = []\n",
    "y_filtered = []\n",
    "for i in range(len(X)):\n",
    "    if y[i] <= 1:\n",
    "        X_filtered.append(X[i])\n",
    "        y_filtered.append(y[i])\n",
    "\n",
    "for i in range(len(X_filtered)):\n",
    "    if y_filtered[i] not in [0,1]:\n",
    "        print(X_filtered[i], y_filtered[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)\n",
       "0                5.1               3.5\n",
       "1                4.9               3.0\n",
       "2                4.7               3.2\n",
       "3                4.6               3.1\n",
       "4                5.0               3.6"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# elegir solo sepal length y sepal width\n",
    "X = pd.DataFrame(X_filtered, columns=iris.feature_names)\n",
    "X = X[['sepal length (cm)', 'sepal width (cm)']]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dividir el dataset en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_filtered, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, eta=0.01, n_iter=50):\n",
    "        self.eta = eta  # Tasa de aprendizaje\n",
    "        self.n_iter = n_iter # Número de iteraciones\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.w_ = np.zeros(1 + X.shape[1])  # Inicializar los pesos\n",
    "        self.errors_ = []  # Número de errores en cada iteración\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            errors = 0\n",
    "            for xi, target in zip(X, y):\n",
    "                update = self.eta * (target - self.predict(xi))\n",
    "                self.w_[1:] += update * xi\n",
    "                self.w_[0] += update\n",
    "                errors += int(update != 0.0)\n",
    "            self.errors_.append(errors)\n",
    "        return self\n",
    "\n",
    "    def net_input(self, X):\n",
    "        \"\"\"Calcular la entrada neta, es decir, el producto punto entre X y los pesos más el sesgo\"\"\"\n",
    "        return np.dot(X, self.w_[1:]) + self.w_[0]\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Devolver la etiqueta de clase usando la función escalón unitario\"\"\"\n",
    "        return np.where(self.net_input(X) >= 0.0, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Perceptron at 0x296bc1850>"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron = Perceptron(eta=0.1, n_iter=1000)\n",
    "perceptron.fit(X_train.values, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.plot(range(1, len(perceptron.errors_) + 1), perceptron.errors_, marker='o')\n",
    "plt.xlabel('Iteraciones')\n",
    "plt.ylabel('Número de errores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.4\n"
     ]
    }
   ],
   "source": [
    "# hacer predicciones\n",
    "y_pred = perceptron.predict(X_test.values)\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        12\n",
      "           1       0.40      1.00      0.57         8\n",
      "\n",
      "    accuracy                           0.40        20\n",
      "   macro avg       0.20      0.50      0.29        20\n",
      "weighted avg       0.16      0.40      0.23        20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusión\n",
    "\n",
    "Utilizamos accuracy como métrica de desempeño ya que los datos estaban balanceados. Debido a que obtuvimos un accuracy de 0.40 determinamos que "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
